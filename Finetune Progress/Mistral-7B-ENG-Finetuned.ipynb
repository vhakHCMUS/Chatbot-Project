{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-25T16:36:25.995310Z",
     "iopub.status.busy": "2024-12-25T16:36:25.994453Z",
     "iopub.status.idle": "2024-12-25T16:37:23.664603Z",
     "shell.execute_reply": "2024-12-25T16:37:23.663480Z",
     "shell.execute_reply.started": "2024-12-25T16:36:25.995271Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U bitsandbytes\n",
    "%pip install -U transformers\n",
    "%pip install -U peft\n",
    "%pip install -U accelerate\n",
    "%pip install -U trl \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:37:23.666478Z",
     "iopub.status.busy": "2024-12-25T16:37:23.666196Z",
     "iopub.status.idle": "2024-12-25T16:37:42.409747Z",
     "shell.execute_reply": "2024-12-25T16:37:42.408847Z",
     "shell.execute_reply.started": "2024-12-25T16:37:23.666451Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\lib\\site-packages\\transformers\\utils\\hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:37:43.013221Z",
     "iopub.status.busy": "2024-12-25T16:37:43.012947Z",
     "iopub.status.idle": "2024-12-25T16:37:44.964267Z",
     "shell.execute_reply": "2024-12-25T16:37:44.963376Z",
     "shell.execute_reply.started": "2024-12-25T16:37:43.013195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:38:23.720738Z",
     "iopub.status.busy": "2024-12-25T16:38:23.720099Z",
     "iopub.status.idle": "2024-12-25T16:38:28.467039Z",
     "shell.execute_reply": "2024-12-25T16:38:28.466269Z",
     "shell.execute_reply.started": "2024-12-25T16:38:23.720704Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: bonpaul (bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm). Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Admin\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Chatbot-Project\\wandb\\run-20241230_225221-tsbdqeix</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B/runs/tsbdqeix' target=\"_blank\">faithful-elevator-38</a></strong> to <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B' target=\"_blank\">https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B/runs/tsbdqeix' target=\"_blank\">https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B/runs/tsbdqeix</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Monitering the LLM\n",
    "wandb.login(key = \"ec7cc0ef28b4a745b45ed281f881bfee6399aed7\")\n",
    "run = wandb.init(\n",
    "    project='Fine tuning mistral 7B', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:38:32.611127Z",
     "iopub.status.busy": "2024-12-25T16:38:32.610241Z",
     "iopub.status.idle": "2024-12-25T16:38:32.614991Z",
     "shell.execute_reply": "2024-12-25T16:38:32.614240Z",
     "shell.execute_reply.started": "2024-12-25T16:38:32.611080Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = \"meta-math/MetaMath-Mistral-7B\"  # Hugging Face model repo\n",
    "#dataset_name = \"tranthaihoa/math_test\"\n",
    "new_model = \"mistral_7b_eng-math\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:38:34.463147Z",
     "iopub.status.busy": "2024-12-25T16:38:34.462815Z",
     "iopub.status.idle": "2024-12-25T16:38:37.012735Z",
     "shell.execute_reply": "2024-12-25T16:38:37.011860Z",
     "shell.execute_reply.started": "2024-12-25T16:38:34.463118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 821\n",
      "Train set size: 656\n",
      "Validation set size: 82\n",
      "Test set size: 83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72905d1a52c49b1aa64441ea8dda634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057fdaf713064cbf8e5b8da944e4568a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37899535af842248becd9e4b5f1350e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/83 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved to D:\\cache\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "# Dataset details\n",
    "dataset_name = \"tranthaihoa/math_test\"\n",
    "output_dir = Path(\"D:\\cache\")  # Specify output directory for saving datasets\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_meta_math(batch):\n",
    "    result = {\n",
    "        \"instruction\": [],\n",
    "        \"response\": []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(batch[\"id\"])):\n",
    "        # Construct instruction and response\n",
    "        instruction = (\n",
    "            f\"[ID: {batch['id'][i]}] \"\n",
    "            f\"[Question: {batch['Question'][i]}] \"\n",
    "            f\"[Explanation: {batch['Explanation'][i]}] \"\n",
    "            f\"[Inference Steps: {batch['Inference Steps'][i]}] \"\n",
    "            f\"[Grade: {batch['Grade'][i]}] \"\n",
    "            f\"[Source: {batch['Source'][i]}] \"\n",
    "            f\"[Instruction: {batch['Instruction'][i]}] \"\n",
    "            f\"[Response Type: {batch['Response Type'][i]}] \"\n",
    "            f\"[Math Type: {batch['Math Type'][i]}]\"\n",
    "        )\n",
    "        response = batch[\"Answer\"][i]\n",
    "\n",
    "        result[\"instruction\"].append(instruction)\n",
    "        result[\"response\"].append(response)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "ds = load_dataset(dataset_name)\n",
    "preprocessed_ds = ds.map(preprocess_meta_math, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffled_dataset = preprocessed_ds[\"train\"].shuffle(seed=42)\n",
    "\n",
    "# Limit to the total rows in the dataset\n",
    "total_rows = 821\n",
    "limited_dataset = shuffled_dataset.select(range(total_rows))\n",
    "\n",
    "# Calculate split sizes (80% train, 10% validation, 10% test)\n",
    "train_size = int(0.8 * total_rows)  # 656 rows for train\n",
    "val_size = int(0.1 * total_rows)    # 82 rows for validation\n",
    "test_size = total_rows - train_size - val_size  # 83 rows for test\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = limited_dataset.select(range(train_size))\n",
    "val_dataset = limited_dataset.select(range(train_size, train_size + val_size))\n",
    "test_dataset = limited_dataset.select(range(train_size + val_size, total_rows))\n",
    "\n",
    "# Print the sizes of each split\n",
    "print(f\"Total dataset size: {total_rows}\")\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "# Save splits in Hugging Face Dataset format\n",
    "train_dataset.save_to_disk(\"D:/cache/train_dataset\")\n",
    "val_dataset.save_to_disk(\"D:/cache/validation_dataset\")\n",
    "test_dataset.save_to_disk(\"D:/cache/test_dataset\")\n",
    "\n",
    "print(f\"Datasets saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:38:41.984287Z",
     "iopub.status.busy": "2024-12-25T16:38:41.983656Z",
     "iopub.status.idle": "2024-12-25T16:38:41.996265Z",
     "shell.execute_reply": "2024-12-25T16:38:41.995569Z",
     "shell.execute_reply.started": "2024-12-25T16:38:41.984246Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "train_dataset = load_from_disk(\"D:/cache/translated_train_dataset\")\n",
    "val_dataset = load_from_disk(\"D:/cache/translated_validation_dataset\")\n",
    "test_dataset = load_from_disk(\"D:/cache/translated_test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:52:58.811901Z",
     "iopub.status.busy": "2024-12-25T16:52:58.811576Z",
     "iopub.status.idle": "2024-12-25T16:53:20.806509Z",
     "shell.execute_reply": "2024-12-25T16:53:20.805602Z",
     "shell.execute_reply.started": "2024-12-25T16:52:58.811875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941f26fce469405bbfbfb88640429497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from safetensors.torch import load_file  # Import for loading safetensors\n",
    "\n",
    "# Configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Define the base model and adapter checkpoint paths\n",
    "base_model_name = base_model  # Replace with the actual base model name\n",
    "#adapter_checkpoint_path = \"D:/Chatbot-Project/fine-tuned-model/checkpoint-2500/adapter_model.safetensors\"\n",
    "\n",
    "# Load the base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load adapter weights using safetensors\n",
    "#adapter_state_dict = load_file(adapter_checkpoint_path)\n",
    "#model.load_state_dict(adapter_state_dict, strict=False)  # Load adapter into the model\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:54:12.612508Z",
     "iopub.status.busy": "2024-12-25T16:54:12.612153Z",
     "iopub.status.idle": "2024-12-25T16:54:13.285471Z",
     "shell.execute_reply": "2024-12-25T16:54:13.284799Z",
     "shell.execute_reply.started": "2024-12-25T16:54:12.612477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d92328879540c58f11590dddf6763e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648f72f9f8ed49d0ba68216ce527f003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(\n",
    "        examples[\"instruction\"],  # Use preformatted 'instruction' as the input\n",
    "        text_pair=examples[\"response\"],  # Use 'response' for paired input-output sequences\n",
    "        padding=\"max_length\",  # Pad sequences to the maximum length\n",
    "        truncation=True,       # Truncate sequences longer than the max length\n",
    "        max_length=512,        # Set the maximum length for tokenization\n",
    "    )\n",
    "\n",
    "# Tokenize the train and validation datasets\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True  # Process multiple examples at once for efficiency\n",
    ")\n",
    "\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True  # Similarly process the validation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:54:17.835053Z",
     "iopub.status.busy": "2024-12-25T16:54:17.834343Z",
     "iopub.status.idle": "2024-12-25T16:54:17.863618Z",
     "shell.execute_reply": "2024-12-25T16:54:17.862746Z",
     "shell.execute_reply.started": "2024-12-25T16:54:17.835022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32001, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:54:22.098060Z",
     "iopub.status.busy": "2024-12-25T16:54:22.097424Z",
     "iopub.status.idle": "2024-12-25T16:54:23.219000Z",
     "shell.execute_reply": "2024-12-25T16:54:23.217936Z",
     "shell.execute_reply.started": "2024-12-25T16:54:22.098029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041184 || all params: 3837120544 || trainable%: 2.216276059739029\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:54:26.560933Z",
     "iopub.status.busy": "2024-12-25T16:54:26.560588Z",
     "iopub.status.idle": "2024-12-25T16:54:26.565311Z",
     "shell.execute_reply": "2024-12-25T16:54:26.564300Z",
     "shell.execute_reply.started": "2024-12-25T16:54:26.560903Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:54:29.562157Z",
     "iopub.status.busy": "2024-12-25T16:54:29.561814Z",
     "iopub.status.idle": "2024-12-25T16:54:29.613371Z",
     "shell.execute_reply": "2024-12-25T16:54:29.612703Z",
     "shell.execute_reply.started": "2024-12-25T16:54:29.562128Z"
    }
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# Initialize the accelerator\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:54:32.301021Z",
     "iopub.status.busy": "2024-12-25T16:54:32.300203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps per epoch: 164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb78d1a82fe4646a3a9fb69c138afdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/984 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9756, 'grad_norm': 2.1959965229034424, 'learning_rate': 2.319004524886878e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4825a9231e6943319c57d87c9cc0380e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7124689817428589, 'eval_runtime': 39.6156, 'eval_samples_per_second': 2.07, 'eval_steps_per_second': 0.278, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.594, 'grad_norm': 2.400202751159668, 'learning_rate': 1.8552036199095023e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ce3abdef1e4d75aeafdc0afffe589f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6864623427391052, 'eval_runtime': 39.6039, 'eval_samples_per_second': 2.071, 'eval_steps_per_second': 0.278, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.44, 'grad_norm': 2.7894482612609863, 'learning_rate': 1.3914027149321268e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f9269b51514093834e0fb77fb81ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7204725742340088, 'eval_runtime': 39.5219, 'eval_samples_per_second': 2.075, 'eval_steps_per_second': 0.278, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.299, 'grad_norm': 4.15683126449585, 'learning_rate': 9.276018099547511e-06, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e0293e753342cd875c264bc651ce39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.80153888463974, 'eval_runtime': 39.4486, 'eval_samples_per_second': 2.079, 'eval_steps_per_second': 0.279, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1902, 'grad_norm': 4.116815090179443, 'learning_rate': 4.638009049773756e-06, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1330e63f714439bce0e24df6a38643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9139918684959412, 'eval_runtime': 39.4457, 'eval_samples_per_second': 2.079, 'eval_steps_per_second': 0.279, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1231, 'grad_norm': 5.296316623687744, 'learning_rate': 0.0, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f63f377fbd4abd8793a2bb5e561ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0480365753173828, 'eval_runtime': 39.3995, 'eval_samples_per_second': 2.081, 'eval_steps_per_second': 0.279, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\lib\\site-packages\\peft\\utils\\save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6961.0432, 'train_samples_per_second': 0.565, 'train_steps_per_second': 0.141, 'train_loss': 0.4369705188565138, 'epoch': 6.0}\n",
      "Model and tokenizer have been saved to: D:\\cache\\mistral-fine-tuned-mistral-eng-math-finetune\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback\n",
    "import transformers\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Paths and project configuration\n",
    "cache_dir = Path(\"D:/cache\")  # Kaggle's writable directory\n",
    "project = \"eng-math-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = f\"{base_model_name}-{project}\"\n",
    "output_dir = cache_dir / f\"mistral-fine-tuned-{run_name}\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset parameters\n",
    "num_train_samples = 656  # Train set size\n",
    "num_val_samples = 82     # Validation set size\n",
    "num_test_samples = 83    # Test set size\n",
    "batch_size = 4           # Set a larger batch size\n",
    "num_epochs = 6           # Set number of epochs to 6\n",
    "\n",
    "# Calculate steps\n",
    "steps_per_epoch = (num_train_samples + batch_size - 1) // batch_size  # Round up\n",
    "total_steps = steps_per_epoch * num_epochs\n",
    "print(\"steps per epoch:\", steps_per_epoch)\n",
    "\n",
    "# Define optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2.5e-5, weight_decay=0.1)\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=100,  # Adjust warmup steps to fit the smaller dataset\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=1,\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=total_steps,  # Set calculated total steps\n",
    "        learning_rate=2.5e-5,\n",
    "        weight_decay=0.1,  # Apply weight decay\n",
    "        bf16=True,  # Enable bf16 if supported\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=steps_per_epoch,  # Log once per epoch\n",
    "        logging_dir=str(cache_dir / \"logs\"),\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=steps_per_epoch,  # Save every epoch\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=steps_per_epoch,  # Evaluate every epoch\n",
    "        do_eval=True,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    optimizers=(optimizer, None),  # Pass the optimizer with weight decay\n",
    ")\n",
    "\n",
    "# Disable cache for training\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Model and tokenizer have been saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6373386,
     "sourceId": 10297125,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 142909379,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 1902,
     "modelInstanceId": 3899,
     "sourceId": 5111,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
