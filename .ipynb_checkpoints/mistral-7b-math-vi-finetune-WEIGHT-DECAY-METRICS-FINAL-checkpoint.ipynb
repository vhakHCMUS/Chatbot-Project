{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10297125,"sourceType":"datasetVersion","datasetId":6373386},{"sourceId":142909379,"sourceType":"kernelVersion"},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899,"modelId":1902}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl \nprint(\"Done\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:32:05.894309Z","iopub.execute_input":"2024-12-28T06:32:05.894609Z","iopub.status.idle":"2024-12-28T06:33:03.520866Z","shell.execute_reply.started":"2024-12-28T06:32:05.894582Z","shell.execute_reply":"2024-12-28T06:33:03.519756Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nCollecting transformers\n  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.3\n    Uninstalling tokenizers-0.20.3:\n      Successfully uninstalled tokenizers-0.20.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.46.3\n    Uninstalling transformers-4.46.3:\n      Successfully uninstalled transformers-4.46.3\nSuccessfully installed tokenizers-0.21.0 transformers-4.47.1\nNote: you may need to restart the kernel to use updated packages.\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.47.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m364.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.14.0\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.1.1)\nCollecting accelerate\n  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.26.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.1.1\n    Uninstalling accelerate-1.1.1:\n      Successfully uninstalled accelerate-1.1.1\nSuccessfully installed accelerate-1.2.1\nNote: you may need to restart the kernel to use updated packages.\nCollecting trl\n  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from trl) (1.2.1)\nRequirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl) (3.1.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nRequirement already satisfied: transformers>=4.46.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.47.1)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.26.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.9.5)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (2024.5.15)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (0.21.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.34.0->trl) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.6.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\nDownloading trl-0.13.0-py3-none-any.whl (293 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl\nSuccessfully installed trl-0.13.0\nNote: you may need to restart the kernel to use updated packages.\nDone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:33:03.523017Z","iopub.execute_input":"2024-12-28T06:33:03.523785Z","iopub.status.idle":"2024-12-28T06:33:22.275897Z","shell.execute_reply.started":"2024-12-28T06:33:03.523733Z","shell.execute_reply":"2024-12-28T06:33:22.275215Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:33:22.276816Z","iopub.execute_input":"2024-12-28T06:33:22.277325Z","iopub.status.idle":"2024-12-28T06:33:22.747293Z","shell.execute_reply.started":"2024-12-28T06:33:22.277298Z","shell.execute_reply":"2024-12-28T06:33:22.746161Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:33:22.750014Z","iopub.execute_input":"2024-12-28T06:33:22.750390Z","iopub.status.idle":"2024-12-28T06:33:24.295345Z","shell.execute_reply.started":"2024-12-28T06:33:22.750350Z","shell.execute_reply":"2024-12-28T06:33:24.294456Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `vhak` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `vhak`\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Monitering the LLM\nwandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning mistral 7B', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:33:24.296685Z","iopub.execute_input":"2024-12-28T06:33:24.296985Z","iopub.status.idle":"2024-12-28T06:33:27.126358Z","shell.execute_reply.started":"2024-12-28T06:33:24.296956Z","shell.execute_reply":"2024-12-28T06:33:27.125665Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbonpaul\u001b[0m (\u001b[33mbonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241228_063325-qxfa7255</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B/runs/qxfa7255' target=\"_blank\">flowing-blaze-31</a></strong> to <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B' target=\"_blank\">https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B/runs/qxfa7255' target=\"_blank\">https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20mistral%207B/runs/qxfa7255</a>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"base_model = \"meta-math/MetaMath-Mistral-7B\"  # Hugging Face model repo\n\ndataset_name = \"tranthaihoa/math_test\"\nnew_model = \"mistral_7b_vi-math_v2\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:33:27.127435Z","iopub.execute_input":"2024-12-28T06:33:27.127801Z","iopub.status.idle":"2024-12-28T06:33:27.132225Z","shell.execute_reply.started":"2024-12-28T06:33:27.127754Z","shell.execute_reply":"2024-12-28T06:33:27.131402Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\nfrom pathlib import Path\n\n# Dataset details\ndataset_name = \"tranthaihoa/math_test\"\noutput_dir = Path(\"/kaggle/working\")  # Specify output directory for saving datasets\n\n# Preprocessing function\ndef preprocess_meta_math(batch):\n    result = {\n        \"instruction\": [],\n        \"response\": []\n    }\n    \n    for i in range(len(batch[\"id\"])):\n        # Construct instruction and response\n        instruction = (\n            f\"[ID: {batch['id'][i]}] \"\n            f\"[Question: {batch['Question'][i]}] \"\n            f\"[Explanation: {batch['Explanation'][i]}] \"\n            f\"[Inference Steps: {batch['Inference Steps'][i]}] \"\n            f\"[Grade: {batch['Grade'][i]}] \"\n            f\"[Source: {batch['Source'][i]}] \"\n            f\"[Instruction: {batch['Instruction'][i]}] \"\n            f\"[Response Type: {batch['Response Type'][i]}] \"\n            f\"[Math Type: {batch['Math Type'][i]}]\"\n        )\n        response = batch[\"Answer\"][i]\n\n        result[\"instruction\"].append(instruction)\n        result[\"response\"].append(response)\n    \n    return result\n\n# Load and preprocess the dataset\nds = load_dataset(dataset_name)\npreprocessed_ds = ds.map(preprocess_meta_math, batched=True, remove_columns=ds[\"train\"].column_names)\n\n# Shuffle the dataset\nshuffled_dataset = preprocessed_ds[\"train\"].shuffle(seed=42)\n\n# Limit to the total rows in the dataset\ntotal_rows = 821\nlimited_dataset = shuffled_dataset.select(range(total_rows))\n\n# Calculate split sizes (80% train, 10% validation, 10% test)\ntrain_size = int(0.8 * total_rows)  # 656 rows for train\nval_size = int(0.1 * total_rows)    # 82 rows for validation\ntest_size = total_rows - train_size - val_size  # 83 rows for test\n\n# Split the dataset\ntrain_dataset = limited_dataset.select(range(train_size))\nval_dataset = limited_dataset.select(range(train_size, train_size + val_size))\ntest_dataset = limited_dataset.select(range(train_size + val_size, total_rows))\n\n# Print the sizes of each split\nprint(f\"Total dataset size: {total_rows}\")\nprint(f\"Train set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(val_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\n\n# Save splits in Hugging Face Dataset format\ntrain_dataset.save_to_disk(output_dir / \"train_dataset\")\nval_dataset.save_to_disk(output_dir / \"validation_dataset\")\ntest_dataset.save_to_disk(output_dir / \"test_dataset\")\n\nprint(f\"Datasets saved to {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:33:27.133340Z","iopub.execute_input":"2024-12-28T06:33:27.133589Z","iopub.status.idle":"2024-12-28T06:33:29.612636Z","shell.execute_reply.started":"2024-12-28T06:33:27.133565Z","shell.execute_reply":"2024-12-28T06:33:29.611803Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e8ce239aea14ba6bc05cef0e5a61992"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/330k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e003c47fdf1c42d4a00d48c0dbc64170"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee004f4e9be464488a710ec1a7e8f82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0725c530a3ce4a39a41797d387de272e"}},"metadata":{}},{"name":"stdout","text":"Total dataset size: 821\nTrain set size: 656\nValidation set size: 82\nTest set size: 83\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/656 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"743b0ebe6a2846f0951f71947dbaef5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/82 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c858d29e5175410bac73243e4a288bfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/83 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ba7ee7dfcbc49d585c63b01db309f26"}},"metadata":{}},{"name":"stdout","text":"Datasets saved to /kaggle/working\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"FIXXXXXXXXX","metadata":{}},{"cell_type":"code","source":"from datasets import load_from_disk\n\noutput_dir = Path(\"/kaggle/working\")\n\ntrain_dataset = load_from_disk(output_dir / \"train_dataset\")\nval_dataset = load_from_disk(output_dir / \"validation_dataset\")\ntest_dataset = load_from_disk(output_dir / \"test_dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:33:29.613647Z","iopub.execute_input":"2024-12-28T06:33:29.613898Z","iopub.status.idle":"2024-12-28T06:33:29.626851Z","shell.execute_reply.started":"2024-12-28T06:33:29.613873Z","shell.execute_reply":"2024-12-28T06:33:29.626002Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom safetensors.torch import load_file  # Import for loading safetensors\n\n# Configuration for 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Define the base model and adapter checkpoint paths\nbase_model_name = base_model  # Replace with the actual base model name\nadapter_checkpoint_path = \"/kaggle/input/checkpoint/checkpoint-2500/adapter_model.safetensors\"\n\n# Load the base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n# Load adapter weights using safetensors\nadapter_state_dict = load_file(adapter_checkpoint_path)\nmodel.load_state_dict(adapter_state_dict, strict=False)  # Load adapter into the model\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\nprint(\"Model and tokenizer loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:33:29.627866Z","iopub.execute_input":"2024-12-28T06:33:29.628104Z","iopub.status.idle":"2024-12-28T06:39:57.226689Z","shell.execute_reply.started":"2024-12-28T06:33:29.628080Z","shell.execute_reply":"2024-12-28T06:39:57.225825Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2f2afea02394732a7311ac283eb1cf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6be59b2a1fea41b3b5bae0ee99132672"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fd4058838a44cf9ab9d0d0ac2df95cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92bc4357a5dc4bb8addbab29a2c53107"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f0f164f0b1f492e8ca1a405e229ac8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ec882374ef424da20f6e1febead451"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39f414e2e81e4a9199d746cc70ad0d74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb58a84fb9a44959adc0db9c1bb373db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827c3da257714a1aa64276209efa71d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d319b025274678b1549829e744e885"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89f3954b61a0442382e419b0be911b67"}},"metadata":{}},{"name":"stdout","text":"Model and tokenizer loaded successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    padding_side=\"left\",\n    add_eos_token=True,\n    add_bos_token=True,\n)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n\n    return tokenizer(\n        examples[\"instruction\"],  # Use preformatted 'instruction' as the input\n        text_pair=examples[\"response\"],  # Use 'response' for paired input-output sequences\n        padding=\"max_length\",  # Pad sequences to the maximum length\n        truncation=True,       # Truncate sequences longer than the max length\n        max_length=512,        # Set the maximum length for tokenization\n    )\n\n# Tokenize the train and validation datasets\ntokenized_train_dataset = train_dataset.map(\n    tokenize_function, \n    batched=True  # Process multiple examples at once for efficiency\n)\n\ntokenized_val_dataset = val_dataset.map(\n    tokenize_function, \n    batched=True  # Similarly process the validation dataset\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:39:57.229919Z","iopub.execute_input":"2024-12-28T06:39:57.230275Z","iopub.status.idle":"2024-12-28T06:39:58.131678Z","shell.execute_reply.started":"2024-12-28T06:39:57.230247Z","shell.execute_reply":"2024-12-28T06:39:58.130854Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/656 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c3e04ae9f3a4b5495150fb568cbb7f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/82 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"173485aadb504ee7ae72390b6ce58915"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:39:58.132913Z","iopub.execute_input":"2024-12-28T06:39:58.133271Z","iopub.status.idle":"2024-12-28T06:39:58.160755Z","shell.execute_reply.started":"2024-12-28T06:39:58.133232Z","shell.execute_reply":"2024-12-28T06:39:58.159982Z"}},"outputs":[{"name":"stdout","text":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32001, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:39:58.161893Z","iopub.execute_input":"2024-12-28T06:39:58.162505Z","iopub.status.idle":"2024-12-28T06:39:59.330333Z","shell.execute_reply.started":"2024-12-28T06:39:58.162454Z","shell.execute_reply":"2024-12-28T06:39:59.329567Z"}},"outputs":[{"name":"stdout","text":"trainable params: 85041184 || all params: 3837120544 || trainable%: 2.216276059739029\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"if torch.cuda.device_count() > 1: # If more than 1 GPU\n    model.is_parallelizable = True\n    model.model_parallel = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:39:59.331439Z","iopub.execute_input":"2024-12-28T06:39:59.331735Z","iopub.status.idle":"2024-12-28T06:39:59.336748Z","shell.execute_reply.started":"2024-12-28T06:39:59.331697Z","shell.execute_reply":"2024-12-28T06:39:59.335892Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from accelerate import Accelerator\n\n# Initialize the accelerator\naccelerator = Accelerator()\nmodel = accelerator.prepare_model(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:39:59.337822Z","iopub.execute_input":"2024-12-28T06:39:59.338085Z","iopub.status.idle":"2024-12-28T06:39:59.390775Z","shell.execute_reply.started":"2024-12-28T06:39:59.338057Z","shell.execute_reply":"2024-12-28T06:39:59.390098Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"!pip install evaluate\n\nfrom pathlib import Path\nfrom transformers import Trainer, TrainingArguments, TrainerCallback\nimport transformers\nimport torch\nfrom datetime import datetime\nimport shutil\nfrom IPython.display import FileLink\nimport evaluate\n\n# Paths and project configuration\ncache_dir = Path(\"/kaggle/working\")  # Kaggle's writable directory\nproject = \"vi-math-finetune\"\nbase_model_name = \"mistral\"\nrun_name = f\"{base_model_name}-{project}\"\noutput_dir = cache_dir / f\"mistral-fine-tuned-{run_name}\"\n\n# Ensure the output directory exists\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Dataset parameters\nnum_train_samples = 656  # Train set size\nnum_val_samples = 82     # Validation set size\nnum_test_samples = 83    # Test set size\nbatch_size = 10           # Set a larger batch size\nnum_epochs = 6           # Set number of epochs to 6\n\n# Calculate steps\nsteps_per_epoch = (num_train_samples + batch_size - 1) // batch_size  # Round up\ntotal_steps = steps_per_epoch * num_epochs\nprint(\"steps per epoch:\", steps_per_epoch)\n\n# Custom callback for weight decay adjustment and logging\nclass CustomWeightDecayCallback(TrainerCallback):\n    def __init__(self, optimizer, initial_weight_decay=0.01, decay_factor=0.9):\n        self.optimizer = optimizer\n        self.weight_decay = initial_weight_decay\n        self.decay_factor = decay_factor\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        # Adjust weight decay\n        self.weight_decay *= self.decay_factor\n        for group in self.optimizer.param_groups:\n            group['weight_decay'] = self.weight_decay\n\n        # Log current learning rate and weight decay\n        current_lr = self.optimizer.param_groups[0]['lr']\n        print(f\"Epoch {state.epoch}: Learning Rate: {current_lr:.6f}, Weight Decay: {self.weight_decay:.6f}\")\n        \n        # Optional: Log first layer weights for monitoring\n        first_layer_weights = next(iter(self.optimizer.param_groups[0]['params'])).data\n        weight_norm = torch.norm(first_layer_weights).item()\n        print(f\"Epoch {state.epoch}: First Layer Weight Norm: {weight_norm:.6f}\")\n\n# Metric definitions\nmetric = evaluate.load(\"squad\")  # or other metrics like exact_match or f1\n\ndef compute_metrics(p):\n    # p.predictions contains the model's predictions, and p.label_ids contains the ground truth labels\n    predictions, labels = p\n    # Decode the predictions and labels if they are tokenized\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Compute exact match and F1 scores\n    results = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    return results\n\n# Model, tokenizer, and optimizer setup\ntrainer = Trainer(\n    model=model,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    args=TrainingArguments(\n        output_dir=output_dir,\n        warmup_steps=100,  # Adjust warmup steps to fit the smaller dataset\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=1,\n        gradient_checkpointing=True,\n        max_steps=total_steps,  # Set calculated total steps\n        learning_rate=2.5e-5,\n        weight_decay=0.01,  # Initial weight decay\n        bf16=True,  # Enable bf16 if supported\n        optim=\"paged_adamw_8bit\",\n        logging_steps=steps_per_epoch,  # Log once per epoch\n        logging_dir=str(cache_dir / \"logs\"),\n        save_strategy=\"steps\",\n        save_steps=steps_per_epoch,  # Save every epoch\n        evaluation_strategy=\"steps\",  # Evaluate after every epoch\n        eval_steps=steps_per_epoch,  # Evaluate every epoch\n        do_eval=True,\n        report_to=\"wandb\",  # Logs to WandB for monitoring\n        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    compute_metrics=compute_metrics,  # Add the compute_metrics function\n    callbacks=[CustomWeightDecayCallback(optimizer=None)]  # Add custom callback\n)\n\n# Attach the optimizer to the callback\ntrainer.optimizer = torch.optim.AdamW(\n    model.parameters(), lr=2.5e-5, weight_decay=0.01\n)\ntrainer.callback_handler.callbacks[0].optimizer = trainer.optimizer\n\n# Disable cache for training\nmodel.config.use_cache = False\n\n# Start training\ntrainer.train()\n\n# Compress the output directory into a ZIP file\nzip_file_path = f\"{output_dir}.zip\"\nshutil.make_archive(str(output_dir), 'zip', str(output_dir))\nprint(f\"Model files zipped at: {zip_file_path}\")\n\n# Display download link for the zip file\ndisplay(FileLink(zip_file_path))\n\n# Print out the model and tokenizer saving path\nprint(f\"Model and tokenizer saved to {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:52:35.498597Z","iopub.execute_input":"2024-12-28T06:52:35.498940Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nsteps per epoch: 66\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe36f5f857145e3ab0c9db496090179"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c67bdb346bf245d5a14f3193f46db232"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  2/396 : < :, Epoch 0.02/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T06:40:55.640091Z","iopub.status.idle":"2024-12-28T06:40:55.640390Z","shell.execute_reply.started":"2024-12-28T06:40:55.640252Z","shell.execute_reply":"2024-12-28T06:40:55.640267Z"}},"outputs":[],"execution_count":null}]}