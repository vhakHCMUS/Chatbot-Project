{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10297125,"sourceType":"datasetVersion","datasetId":6373386},{"sourceId":142909379,"sourceType":"kernelVersion"},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899,"modelId":1902}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl \nprint(\"Done\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:26:58.311713Z","iopub.execute_input":"2025-01-01T08:26:58.312065Z","iopub.status.idle":"2025-01-01T08:27:56.079269Z","shell.execute_reply.started":"2025-01-01T08:26:58.312030Z","shell.execute_reply":"2025-01-01T08:27:56.078329Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nCollecting transformers\n  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.3\n    Uninstalling tokenizers-0.20.3:\n      Successfully uninstalled tokenizers-0.20.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.46.3\n    Uninstalling transformers-4.46.3:\n      Successfully uninstalled transformers-4.46.3\nSuccessfully installed tokenizers-0.21.0 transformers-4.47.1\nNote: you may need to restart the kernel to use updated packages.\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.47.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.14.0\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.1.1)\nCollecting accelerate\n  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.26.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.1.1\n    Uninstalling accelerate-1.1.1:\n      Successfully uninstalled accelerate-1.1.1\nSuccessfully installed accelerate-1.2.1\nNote: you may need to restart the kernel to use updated packages.\nCollecting trl\n  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from trl) (1.2.1)\nRequirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl) (3.1.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nRequirement already satisfied: transformers>=4.46.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.47.1)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.26.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.9.5)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (2024.5.15)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (0.21.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.34.0->trl) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.6.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\nDownloading trl-0.13.0-py3-none-any.whl (293 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl\nSuccessfully installed trl-0.13.0\nNote: you may need to restart the kernel to use updated packages.\nDone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:27:56.081432Z","iopub.execute_input":"2025-01-01T08:27:56.081707Z","iopub.status.idle":"2025-01-01T08:28:14.890634Z","shell.execute_reply.started":"2025-01-01T08:27:56.081679Z","shell.execute_reply":"2025-01-01T08:28:14.889853Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:14.891951Z","iopub.execute_input":"2025-01-01T08:28:14.892764Z","iopub.status.idle":"2025-01-01T08:28:15.106891Z","shell.execute_reply.started":"2025-01-01T08:28:14.892703Z","shell.execute_reply":"2025-01-01T08:28:15.106193Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:15.108022Z","iopub.execute_input":"2025-01-01T08:28:15.108685Z","iopub.status.idle":"2025-01-01T08:28:16.579656Z","shell.execute_reply.started":"2025-01-01T08:28:15.108644Z","shell.execute_reply":"2025-01-01T08:28:16.578610Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `vhak` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `vhak`\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Monitering the LLM\nwandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning vietcuna', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:16.581134Z","iopub.execute_input":"2025-01-01T08:28:16.581445Z","iopub.status.idle":"2025-01-01T08:28:19.255615Z","shell.execute_reply.started":"2025-01-01T08:28:16.581418Z","shell.execute_reply":"2025-01-01T08:28:19.254958Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbonpaul\u001b[0m (\u001b[33mbonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250101_082817-7mf37p1h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna/runs/7mf37p1h' target=\"_blank\">lively-durian-1</a></strong> to <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna' target=\"_blank\">https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna/runs/7mf37p1h' target=\"_blank\">https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna/runs/7mf37p1h</a>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"base_model = \"Viet-Mistral/Vistral-7B-Chat\"  # Hugging Face model repo\n\ndataset_name = \"tranthaihoa/math_test\"\nnew_model = \"vistral7B-finetuned\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:19.256576Z","iopub.execute_input":"2025-01-01T08:28:19.256825Z","iopub.status.idle":"2025-01-01T08:28:19.261384Z","shell.execute_reply.started":"2025-01-01T08:28:19.256776Z","shell.execute_reply":"2025-01-01T08:28:19.260588Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from datasets import load_dataset, Dataset, load_from_disk\nfrom pathlib import Path\nimport google.generativeai as genai\n\n# Dataset details\ndataset_name = \"tranthaihoa/math_test\"\noutput_dir = Path(\"/kaggle/working\")  # Specify output directory for saving datasets\n\n# Preprocessing function\ndef preprocess_meta_math(batch):\n    result = {\n        \"instruction\": [],\n        \"response\": []\n    }\n    \n    for i in range(len(batch[\"id\"])):\n        # Construct instruction and response\n        instruction = (\n            f\"[ID: {batch['id'][i]}] \"\n            f\"[Question: {batch['Question'][i]}] \"\n            f\"[Explanation: {batch['Explanation'][i]}] \"\n            f\"[Inference Steps: {batch['Inference Steps'][i]}] \"\n            f\"[Grade: {batch['Grade'][i]}] \"\n            f\"[Source: {batch['Source'][i]}] \"\n            f\"[Instruction: {batch['Instruction'][i]}] \"\n            f\"[Response Type: {batch['Response Type'][i]}] \"\n            f\"[Math Type: {batch['Math Type'][i]}]\"\n        )\n        response = batch[\"Answer\"][i]\n\n        result[\"instruction\"].append(instruction)\n        result[\"response\"].append(response)\n    \n    return result\n\n# Load and preprocess the dataset\nprint(\"Loading dataset...\")\nds = load_dataset(dataset_name)\nprint(\"Preprocessing dataset...\")\npreprocessed_ds = ds.map(preprocess_meta_math, batched=True, remove_columns=ds[\"train\"].column_names)\n\n# Shuffle the dataset\nprint(\"Shuffling dataset...\")\nshuffled_dataset = preprocessed_ds[\"train\"].shuffle(seed=42)\n\n# Limit to the total rows in the dataset\ntotal_rows = 821\nlimited_dataset = shuffled_dataset.select(range(total_rows))\n\n# Calculate split sizes (80% train, 10% validation, 10% test)\ntrain_size = int(0.8 * total_rows)  # 656 rows for train\nval_size = int(0.1 * total_rows)    # 82 rows for validation\ntest_size = total_rows - train_size - val_size  # 83 rows for test\n\n# Split the dataset\nprint(\"Splitting dataset...\")\ntrain_dataset = limited_dataset.select(range(train_size))\nval_dataset = limited_dataset.select(range(train_size, train_size + val_size))\ntest_dataset = limited_dataset.select(range(train_size + val_size, total_rows))\n\n# Save splits in Hugging Face Dataset format\nprint(\"Saving datasets to disk...\")\ntrain_dataset.save_to_disk(output_dir / \"train_dataset\")\nval_dataset.save_to_disk(output_dir / \"validation_dataset\")\ntest_dataset.save_to_disk(output_dir / \"test_dataset\")\n\nprint(f\"Datasets saved to {output_dir}\")\nprint(f\"Train set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(val_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\n\n# Print first instruction of the train dataset\nprint(\"\\nPrinting the first instruction from the train dataset:\")\nfirst_instruction = train_dataset[0][\"instruction\"]\nprint(\"First instruction:\", first_instruction)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:19.264552Z","iopub.execute_input":"2025-01-01T08:28:19.265307Z","iopub.status.idle":"2025-01-01T08:28:20.988163Z","shell.execute_reply.started":"2025-01-01T08:28:19.265274Z","shell.execute_reply":"2025-01-01T08:28:20.987278Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d337bbbf8f3c41af8bd52dcef02ef34d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/330k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c01b002e1ef849658a292affcb40e7f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f659a8258ffa41ad8a9982fc9ad3a07a"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82f0cd0638eb4c8f9e23507af961ee7f"}},"metadata":{}},{"name":"stdout","text":"Shuffling dataset...\nSplitting dataset...\nSaving datasets to disk...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/656 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bafade00d3a4c989b0e7401b401b15a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/82 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84e50df4ae6f43e4800308f36b6ef248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/83 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2b7088f0164433287b2066ce2dbc8be"}},"metadata":{}},{"name":"stdout","text":"Datasets saved to /kaggle/working\nTrain set size: 656\nValidation set size: 82\nTest set size: 83\n\nPrinting the first instruction from the train dataset:\nFirst instruction: [ID: 98857616] [Question: hân dịp khai trương, một cửa hàng bánh Pizza giảm giá 10% tất cả các sản phẩm và giảm thêm 5% trên tổng hóa đơn khi mua từ hai sản phẩm trở lên. Bác Lan mua một Pizza rau củ size vừa giá 139 000 đồng và một Pizza thập cẩm size lớn giá 289 000 đồng. Hỏi nếu bác Lan đưa cho nhân viên thu ngân 500 000 đồng thì bác được trả lại bao nhiêu tiền?] [Explanation: Số tiền bác Lan phải trả khi mua 2 bánh pizza được giảm giá 10% là:\n(139000 + 289000) - (139000 + 289000).10% = 385200 (đồng)\nSố tiền bác Lan phải trả sau khi được giảm giá 5% trên tổng hóa đơn là:\n385200 - 385200.5% = 365940 (đồng)\nSố tiền bác Lan được trả lại là:\n500000 - 365940 = 134060 (đồng)] [Inference Steps: 2.0] [Grade: 7.0] [Source: 76_de-thi-giua-hk1-toan-7_Toán 7] [Instruction: Solve the following mathematical problem, ensuring to provide a detailed explanation prior to presenting the final answer.] [Response Type: Numerical] [Math Type: Arithmetic]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"FIXXXXXXXXX","metadata":{}},{"cell_type":"code","source":"from datasets import load_from_disk\n\noutput_dir = Path(\"/kaggle/working\")\n\ntrain_dataset = load_from_disk(output_dir / \"train_dataset\")\nval_dataset = load_from_disk(output_dir / \"validation_dataset\")\ntest_dataset = load_from_disk(output_dir / \"test_dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:20.989231Z","iopub.execute_input":"2025-01-01T08:28:20.989461Z","iopub.status.idle":"2025-01-01T08:28:21.004298Z","shell.execute_reply.started":"2025-01-01T08:28:20.989437Z","shell.execute_reply":"2025-01-01T08:28:21.003497Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom safetensors.torch import load_file  # Import for loading safetensors\n\n# Configuration for 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Define the base model and adapter checkpoint paths\nbase_model_name = base_model  # Replace with the actual base model name\n\n# Load the base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\nprint(\"Model and tokenizer loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:21.005464Z","iopub.execute_input":"2025-01-01T08:28:21.006100Z","iopub.status.idle":"2025-01-01T08:34:52.292694Z","shell.execute_reply.started":"2025-01-01T08:28:21.006060Z","shell.execute_reply":"2025-01-01T08:34:52.291826Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cff9d6123a5f406a8a040183ca848d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1762825d38e64721879c9b81823fd80a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca80f695d3cb40e28a26490268eefa5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb0e44b0f07f48c7a12f305b91c6c46d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7718afcf9d4a0ea5c2bb04f626b030"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68144aad57d14074a5e569dbaf2f3f81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/133 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fb230a8d3e944f0b6a03371a3e3f103"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c4008c6482042908715a2c92297fd8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/597k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46f2ae3867244bd7b44e73d447f8c8b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ce737f54084d64856df00326626120"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff83d3025dfb4dfcae613ec3e964a333"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/169 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3e4b56e124847249edd02a113b56339"}},"metadata":{}},{"name":"stdout","text":"Model and tokenizer loaded successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    padding_side=\"left\",\n    add_eos_token=True,\n    add_bos_token=True,\n)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n\n    return tokenizer(\n        examples[\"instruction\"],  # Use preformatted 'instruction' as the input\n        text_pair=examples[\"response\"],  # Use 'response' for paired input-output sequences\n        padding=\"max_length\",  # Pad sequences to the maximum length\n        truncation=True,       # Truncate sequences longer than the max length\n        max_length=512,        # Set the maximum length for tokenization\n    )\n\n# Tokenize the train and validation datasets\ntokenized_train_dataset = train_dataset.map(\n    tokenize_function, \n    batched=True  # Process multiple examples at once for efficiency\n)\n\ntokenized_val_dataset = val_dataset.map(\n    tokenize_function, \n    batched=True  # Similarly process the validation dataset\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:52.294014Z","iopub.execute_input":"2025-01-01T08:34:52.294706Z","iopub.status.idle":"2025-01-01T08:34:52.836574Z","shell.execute_reply.started":"2025-01-01T08:34:52.294663Z","shell.execute_reply":"2025-01-01T08:34:52.835702Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/656 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf09a0708af14812b3cfa23c793de3ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/82 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddd659715ef645cbbc5a780470b322cc"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:52.837596Z","iopub.execute_input":"2025-01-01T08:34:52.837866Z","iopub.status.idle":"2025-01-01T08:34:52.864659Z","shell.execute_reply.started":"2025-01-01T08:34:52.837839Z","shell.execute_reply":"2025-01-01T08:34:52.863938Z"}},"outputs":[{"name":"stdout","text":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(38369, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=4096, out_features=38369, bias=False)\n)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:52.865548Z","iopub.execute_input":"2025-01-01T08:34:52.865803Z","iopub.status.idle":"2025-01-01T08:34:53.999258Z","shell.execute_reply.started":"2025-01-01T08:34:52.865766Z","shell.execute_reply":"2025-01-01T08:34:53.998371Z"}},"outputs":[{"name":"stdout","text":"trainable params: 85244960 || all params: 3889490976 || trainable%: 2.1916739369239253\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"if torch.cuda.device_count() > 1: # If more than 1 GPU\n    model.is_parallelizable = True\n    model.model_parallel = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:54.000443Z","iopub.execute_input":"2025-01-01T08:34:54.000816Z","iopub.status.idle":"2025-01-01T08:34:54.005760Z","shell.execute_reply.started":"2025-01-01T08:34:54.000759Z","shell.execute_reply":"2025-01-01T08:34:54.004827Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from accelerate import Accelerator\n\n# Initialize the accelerator\naccelerator = Accelerator()\nmodel = accelerator.prepare_model(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:54.006508Z","iopub.execute_input":"2025-01-01T08:34:54.006773Z","iopub.status.idle":"2025-01-01T08:34:54.060201Z","shell.execute_reply.started":"2025-01-01T08:34:54.006745Z","shell.execute_reply":"2025-01-01T08:34:54.059554Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from pathlib import Path\nfrom transformers import Trainer, TrainingArguments\nimport transformers\nfrom datetime import datetime\n\n# Define a custom callback to print train loss\nclass PrintLossCallback(transformers.TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if \"loss\" in logs:\n            print(f\"Train Loss: {logs['loss']}\")\n\ncache_dir = Path(\"F:/ML\")  # Kaggle's writable directory\nproject = \"vi-math-finetune\"\nbase_model_name = \"vietcuna\"\nrun_name = f\"{base_model_name}-{project}\"\noutput_dir = cache_dir / f\"vietcuna-fine-tuned-{run_name}\"\n\n# Ensure the output directory exists\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Dataset parameters\nnum_train_samples = 656  # Train set size\nnum_val_samples = 82     # Validation set size\nnum_test_samples = 83    # Test set size\nbatch_size = 4           # Set a larger batch size\nnum_epochs = 6           # Set number of epochs to 6\n\n# Calculate steps\nsteps_per_epoch = (num_train_samples + batch_size - 1) // batch_size  # Round up\ntotal_steps = steps_per_epoch * num_epochs\nprint(\"steps per epoch:\", steps_per_epoch)\n\n# Define trainer\ntrainer = Trainer(\n    model=model,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    args=TrainingArguments(\n        output_dir=output_dir,\n        warmup_steps=100,  # Adjust warmup steps to fit the smaller dataset\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=1,\n        gradient_checkpointing=True,\n        max_steps=total_steps,  # Set calculated total steps\n        learning_rate=2.5e-5,\n        bf16=True,  # Enable bf16 if supported\n        optim=\"paged_adamw_8bit\",\n        logging_steps=steps_per_epoch,  # Log once per epoch\n        logging_dir=str(cache_dir / \"logs\"),\n        save_strategy=\"steps\",\n        save_steps=steps_per_epoch,  # Save every epoch\n        evaluation_strategy=\"steps\",  # Evaluate after every epoch\n        eval_steps=steps_per_epoch,  # Evaluate every epoch\n        do_eval=True,\n        report_to=\"wandb\",  # Logs to WandB for monitoring\n        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    callbacks=[PrintLossCallback]  # Add the custom callback to print loss\n)\n\n# Disable cache for training\nmodel.config.use_cache = False\n\n# Start training\ntrainer.train()\n\n# Print out the model and tokenizer saving path\nprint(f\"Model and tokenizer saved to {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:54.061304Z","iopub.execute_input":"2025-01-01T08:34:54.061544Z","execution_failed":"2025-01-01T15:16:25.233Z"}},"outputs":[{"name":"stdout","text":"steps per epoch: 164\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='907' max='984' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [907/984 4:47:00 < 24:25, 0.05 it/s, Epoch 5.52/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>164</td>\n      <td>0.933600</td>\n      <td>0.690677</td>\n    </tr>\n    <tr>\n      <td>328</td>\n      <td>0.592800</td>\n      <td>0.661081</td>\n    </tr>\n    <tr>\n      <td>492</td>\n      <td>0.428300</td>\n      <td>0.694747</td>\n    </tr>\n    <tr>\n      <td>656</td>\n      <td>0.283200</td>\n      <td>0.776974</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.178900</td>\n      <td>0.853082</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.9336\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5928\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4283\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2832\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1789\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}