{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10297125,"sourceType":"datasetVersion","datasetId":6373386},{"sourceId":142909379,"sourceType":"kernelVersion"},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899,"modelId":1902}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl \nprint(\"Done\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:26:58.311713Z","iopub.execute_input":"2025-01-01T08:26:58.312065Z","iopub.status.idle":"2025-01-01T08:27:56.079269Z","shell.execute_reply.started":"2025-01-01T08:26:58.312030Z","shell.execute_reply":"2025-01-01T08:27:56.078329Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nCollecting transformers\n  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.3\n    Uninstalling tokenizers-0.20.3:\n      Successfully uninstalled tokenizers-0.20.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.46.3\n    Uninstalling transformers-4.46.3:\n      Successfully uninstalled transformers-4.46.3\nSuccessfully installed tokenizers-0.21.0 transformers-4.47.1\nNote: you may need to restart the kernel to use updated packages.\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.47.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.14.0\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.1.1)\nCollecting accelerate\n  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.26.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.1.1\n    Uninstalling accelerate-1.1.1:\n      Successfully uninstalled accelerate-1.1.1\nSuccessfully installed accelerate-1.2.1\nNote: you may need to restart the kernel to use updated packages.\nCollecting trl\n  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from trl) (1.2.1)\nRequirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl) (3.1.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nRequirement already satisfied: transformers>=4.46.0 in /opt/conda/lib/python3.10/site-packages (from trl) (4.47.1)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.26.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0->trl) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl) (3.9.5)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (2024.5.15)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.46.0->trl) (0.21.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.34.0->trl) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2024.6.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.21.0->trl) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\nDownloading trl-0.13.0-py3-none-any.whl (293 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl\nSuccessfully installed trl-0.13.0\nNote: you may need to restart the kernel to use updated packages.\nDone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:27:56.081432Z","iopub.execute_input":"2025-01-01T08:27:56.081707Z","iopub.status.idle":"2025-01-01T08:28:14.890634Z","shell.execute_reply.started":"2025-01-01T08:27:56.081679Z","shell.execute_reply":"2025-01-01T08:28:14.889853Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:14.891951Z","iopub.execute_input":"2025-01-01T08:28:14.892764Z","iopub.status.idle":"2025-01-01T08:28:15.106891Z","shell.execute_reply.started":"2025-01-01T08:28:14.892703Z","shell.execute_reply":"2025-01-01T08:28:15.106193Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:15.108022Z","iopub.execute_input":"2025-01-01T08:28:15.108685Z","iopub.status.idle":"2025-01-01T08:28:16.579656Z","shell.execute_reply.started":"2025-01-01T08:28:15.108644Z","shell.execute_reply":"2025-01-01T08:28:16.578610Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `vhak` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `vhak`\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Monitering the LLM\nwandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning vietcuna', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:16.581134Z","iopub.execute_input":"2025-01-01T08:28:16.581445Z","iopub.status.idle":"2025-01-01T08:28:19.255615Z","shell.execute_reply.started":"2025-01-01T08:28:16.581418Z","shell.execute_reply":"2025-01-01T08:28:19.254958Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbonpaul\u001b[0m (\u001b[33mbonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250101_082817-7mf37p1h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna/runs/7mf37p1h' target=\"_blank\">lively-durian-1</a></strong> to <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna' target=\"_blank\">https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna/runs/7mf37p1h' target=\"_blank\">https://wandb.ai/bonpaul-tr-ng-i-h-c-khoa-h-c-t-nhi-n-hqg-hcm/Fine%20tuning%20vietcuna/runs/7mf37p1h</a>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"base_model = \"Viet-Mistral/Vistral-7B-Chat\"  # Hugging Face model repo\n\ndataset_name = \"tranthaihoa/math_test\"\nnew_model = \"vistral7B-finetuned\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:19.256576Z","iopub.execute_input":"2025-01-01T08:28:19.256825Z","iopub.status.idle":"2025-01-01T08:28:19.261384Z","shell.execute_reply.started":"2025-01-01T08:28:19.256776Z","shell.execute_reply":"2025-01-01T08:28:19.260588Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from datasets import load_dataset, Dataset, load_from_disk\nfrom pathlib import Path\nimport google.generativeai as genai\n\n# Dataset details\ndataset_name = \"tranthaihoa/math_test\"\noutput_dir = Path(\"/kaggle/working\")  # Specify output directory for saving datasets\n\n# Preprocessing function\ndef preprocess_meta_math(batch):\n    result = {\n        \"instruction\": [],\n        \"response\": []\n    }\n    \n    for i in range(len(batch[\"id\"])):\n        # Construct instruction and response\n        instruction = (\n            f\"[ID: {batch['id'][i]}] \"\n            f\"[Question: {batch['Question'][i]}] \"\n            f\"[Explanation: {batch['Explanation'][i]}] \"\n            f\"[Inference Steps: {batch['Inference Steps'][i]}] \"\n            f\"[Grade: {batch['Grade'][i]}] \"\n            f\"[Source: {batch['Source'][i]}] \"\n            f\"[Instruction: {batch['Instruction'][i]}] \"\n            f\"[Response Type: {batch['Response Type'][i]}] \"\n            f\"[Math Type: {batch['Math Type'][i]}]\"\n        )\n        response = batch[\"Answer\"][i]\n\n        result[\"instruction\"].append(instruction)\n        result[\"response\"].append(response)\n    \n    return result\n\n# Load and preprocess the dataset\nprint(\"Loading dataset...\")\nds = load_dataset(dataset_name)\nprint(\"Preprocessing dataset...\")\npreprocessed_ds = ds.map(preprocess_meta_math, batched=True, remove_columns=ds[\"train\"].column_names)\n\n# Shuffle the dataset\nprint(\"Shuffling dataset...\")\nshuffled_dataset = preprocessed_ds[\"train\"].shuffle(seed=42)\n\n# Limit to the total rows in the dataset\ntotal_rows = 821\nlimited_dataset = shuffled_dataset.select(range(total_rows))\n\n# Calculate split sizes (80% train, 10% validation, 10% test)\ntrain_size = int(0.8 * total_rows)  # 656 rows for train\nval_size = int(0.1 * total_rows)    # 82 rows for validation\ntest_size = total_rows - train_size - val_size  # 83 rows for test\n\n# Split the dataset\nprint(\"Splitting dataset...\")\ntrain_dataset = limited_dataset.select(range(train_size))\nval_dataset = limited_dataset.select(range(train_size, train_size + val_size))\ntest_dataset = limited_dataset.select(range(train_size + val_size, total_rows))\n\n# Save splits in Hugging Face Dataset format\nprint(\"Saving datasets to disk...\")\ntrain_dataset.save_to_disk(output_dir / \"train_dataset\")\nval_dataset.save_to_disk(output_dir / \"validation_dataset\")\ntest_dataset.save_to_disk(output_dir / \"test_dataset\")\n\nprint(f\"Datasets saved to {output_dir}\")\nprint(f\"Train set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(val_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\n\n# Print first instruction of the train dataset\nprint(\"\\nPrinting the first instruction from the train dataset:\")\nfirst_instruction = train_dataset[0][\"instruction\"]\nprint(\"First instruction:\", first_instruction)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:19.264552Z","iopub.execute_input":"2025-01-01T08:28:19.265307Z","iopub.status.idle":"2025-01-01T08:28:20.988163Z","shell.execute_reply.started":"2025-01-01T08:28:19.265274Z","shell.execute_reply":"2025-01-01T08:28:20.987278Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d337bbbf8f3c41af8bd52dcef02ef34d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/330k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c01b002e1ef849658a292affcb40e7f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f659a8258ffa41ad8a9982fc9ad3a07a"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82f0cd0638eb4c8f9e23507af961ee7f"}},"metadata":{}},{"name":"stdout","text":"Shuffling dataset...\nSplitting dataset...\nSaving datasets to disk...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/656 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bafade00d3a4c989b0e7401b401b15a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/82 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84e50df4ae6f43e4800308f36b6ef248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/83 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2b7088f0164433287b2066ce2dbc8be"}},"metadata":{}},{"name":"stdout","text":"Datasets saved to /kaggle/working\nTrain set size: 656\nValidation set size: 82\nTest set size: 83\n\nPrinting the first instruction from the train dataset:\nFirst instruction: [ID: 98857616] [Question: hÃ¢n dá»‹p khai trÆ°Æ¡ng, má»™t cá»­a hÃ ng bÃ¡nh Pizza giáº£m giÃ¡ 10% táº¥t cáº£ cÃ¡c sáº£n pháº©m vÃ  giáº£m thÃªm 5% trÃªn tá»•ng hÃ³a Ä‘Æ¡n khi mua tá»« hai sáº£n pháº©m trá»Ÿ lÃªn. BÃ¡c Lan mua má»™t Pizza rau cá»§ size vá»«a giÃ¡ 139 000 Ä‘á»“ng vÃ  má»™t Pizza tháº­p cáº©m size lá»›n giÃ¡ 289 000 Ä‘á»“ng. Há»i náº¿u bÃ¡c Lan Ä‘Æ°a cho nhÃ¢n viÃªn thu ngÃ¢n 500 000 Ä‘á»“ng thÃ¬ bÃ¡c Ä‘Æ°á»£c tráº£ láº¡i bao nhiÃªu tiá»n?] [Explanation: Sá»‘ tiá»n bÃ¡c Lan pháº£i tráº£ khi mua 2 bÃ¡nh pizza Ä‘Æ°á»£c giáº£m giÃ¡ 10% lÃ :\n(139000 + 289000) - (139000 + 289000).10% = 385200 (Ä‘á»“ng)\nSá»‘ tiá»n bÃ¡c Lan pháº£i tráº£ sau khi Ä‘Æ°á»£c giáº£m giÃ¡ 5% trÃªn tá»•ng hÃ³a Ä‘Æ¡n lÃ :\n385200 - 385200.5% = 365940 (Ä‘á»“ng)\nSá»‘ tiá»n bÃ¡c Lan Ä‘Æ°á»£c tráº£ láº¡i lÃ :\n500000 - 365940 = 134060 (Ä‘á»“ng)] [Inference Steps: 2.0] [Grade: 7.0] [Source: 76_de-thi-giua-hk1-toan-7_ToaÌn 7] [Instruction: Solve the following mathematical problem, ensuring to provide a detailed explanation prior to presenting the final answer.] [Response Type: Numerical] [Math Type: Arithmetic]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"FIXXXXXXXXX","metadata":{}},{"cell_type":"code","source":"from datasets import load_from_disk\n\noutput_dir = Path(\"/kaggle/working\")\n\ntrain_dataset = load_from_disk(output_dir / \"train_dataset\")\nval_dataset = load_from_disk(output_dir / \"validation_dataset\")\ntest_dataset = load_from_disk(output_dir / \"test_dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:20.989231Z","iopub.execute_input":"2025-01-01T08:28:20.989461Z","iopub.status.idle":"2025-01-01T08:28:21.004298Z","shell.execute_reply.started":"2025-01-01T08:28:20.989437Z","shell.execute_reply":"2025-01-01T08:28:21.003497Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom safetensors.torch import load_file  # Import for loading safetensors\n\n# Configuration for 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# Define the base model and adapter checkpoint paths\nbase_model_name = base_model  # Replace with the actual base model name\n\n# Load the base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\nprint(\"Model and tokenizer loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:28:21.005464Z","iopub.execute_input":"2025-01-01T08:28:21.006100Z","iopub.status.idle":"2025-01-01T08:34:52.292694Z","shell.execute_reply.started":"2025-01-01T08:28:21.006060Z","shell.execute_reply":"2025-01-01T08:34:52.291826Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cff9d6123a5f406a8a040183ca848d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1762825d38e64721879c9b81823fd80a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca80f695d3cb40e28a26490268eefa5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb0e44b0f07f48c7a12f305b91c6c46d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7718afcf9d4a0ea5c2bb04f626b030"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68144aad57d14074a5e569dbaf2f3f81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/133 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fb230a8d3e944f0b6a03371a3e3f103"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c4008c6482042908715a2c92297fd8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/597k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46f2ae3867244bd7b44e73d447f8c8b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ce737f54084d64856df00326626120"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff83d3025dfb4dfcae613ec3e964a333"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/169 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3e4b56e124847249edd02a113b56339"}},"metadata":{}},{"name":"stdout","text":"Model and tokenizer loaded successfully!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    base_model,\n    padding_side=\"left\",\n    add_eos_token=True,\n    add_bos_token=True,\n)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n\n    return tokenizer(\n        examples[\"instruction\"],  # Use preformatted 'instruction' as the input\n        text_pair=examples[\"response\"],  # Use 'response' for paired input-output sequences\n        padding=\"max_length\",  # Pad sequences to the maximum length\n        truncation=True,       # Truncate sequences longer than the max length\n        max_length=512,        # Set the maximum length for tokenization\n    )\n\n# Tokenize the train and validation datasets\ntokenized_train_dataset = train_dataset.map(\n    tokenize_function, \n    batched=True  # Process multiple examples at once for efficiency\n)\n\ntokenized_val_dataset = val_dataset.map(\n    tokenize_function, \n    batched=True  # Similarly process the validation dataset\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:52.294014Z","iopub.execute_input":"2025-01-01T08:34:52.294706Z","iopub.status.idle":"2025-01-01T08:34:52.836574Z","shell.execute_reply.started":"2025-01-01T08:34:52.294663Z","shell.execute_reply":"2025-01-01T08:34:52.835702Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/656 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf09a0708af14812b3cfa23c793de3ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/82 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddd659715ef645cbbc5a780470b322cc"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\ndef print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:52.837596Z","iopub.execute_input":"2025-01-01T08:34:52.837866Z","iopub.status.idle":"2025-01-01T08:34:52.864659Z","shell.execute_reply.started":"2025-01-01T08:34:52.837839Z","shell.execute_reply":"2025-01-01T08:34:52.863938Z"}},"outputs":[{"name":"stdout","text":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(38369, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=4096, out_features=38369, bias=False)\n)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:52.865548Z","iopub.execute_input":"2025-01-01T08:34:52.865803Z","iopub.status.idle":"2025-01-01T08:34:53.999258Z","shell.execute_reply.started":"2025-01-01T08:34:52.865766Z","shell.execute_reply":"2025-01-01T08:34:53.998371Z"}},"outputs":[{"name":"stdout","text":"trainable params: 85244960 || all params: 3889490976 || trainable%: 2.1916739369239253\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"if torch.cuda.device_count() > 1: # If more than 1 GPU\n    model.is_parallelizable = True\n    model.model_parallel = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:54.000443Z","iopub.execute_input":"2025-01-01T08:34:54.000816Z","iopub.status.idle":"2025-01-01T08:34:54.005760Z","shell.execute_reply.started":"2025-01-01T08:34:54.000759Z","shell.execute_reply":"2025-01-01T08:34:54.004827Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from accelerate import Accelerator\n\n# Initialize the accelerator\naccelerator = Accelerator()\nmodel = accelerator.prepare_model(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:54.006508Z","iopub.execute_input":"2025-01-01T08:34:54.006773Z","iopub.status.idle":"2025-01-01T08:34:54.060201Z","shell.execute_reply.started":"2025-01-01T08:34:54.006745Z","shell.execute_reply":"2025-01-01T08:34:54.059554Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from pathlib import Path\nfrom transformers import Trainer, TrainingArguments\nimport transformers\nfrom datetime import datetime\n\n# Define a custom callback to print train loss\nclass PrintLossCallback(transformers.TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if \"loss\" in logs:\n            print(f\"Train Loss: {logs['loss']}\")\n\ncache_dir = Path(\"F:/ML\")  # Kaggle's writable directory\nproject = \"vi-math-finetune\"\nbase_model_name = \"vietcuna\"\nrun_name = f\"{base_model_name}-{project}\"\noutput_dir = cache_dir / f\"vietcuna-fine-tuned-{run_name}\"\n\n# Ensure the output directory exists\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Dataset parameters\nnum_train_samples = 656  # Train set size\nnum_val_samples = 82     # Validation set size\nnum_test_samples = 83    # Test set size\nbatch_size = 4           # Set a larger batch size\nnum_epochs = 6           # Set number of epochs to 6\n\n# Calculate steps\nsteps_per_epoch = (num_train_samples + batch_size - 1) // batch_size  # Round up\ntotal_steps = steps_per_epoch * num_epochs\nprint(\"steps per epoch:\", steps_per_epoch)\n\n# Define trainer\ntrainer = Trainer(\n    model=model,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    args=TrainingArguments(\n        output_dir=output_dir,\n        warmup_steps=100,  # Adjust warmup steps to fit the smaller dataset\n        per_device_train_batch_size=batch_size,\n        gradient_accumulation_steps=1,\n        gradient_checkpointing=True,\n        max_steps=total_steps,  # Set calculated total steps\n        learning_rate=2.5e-5,\n        bf16=True,  # Enable bf16 if supported\n        optim=\"paged_adamw_8bit\",\n        logging_steps=steps_per_epoch,  # Log once per epoch\n        logging_dir=str(cache_dir / \"logs\"),\n        save_strategy=\"steps\",\n        save_steps=steps_per_epoch,  # Save every epoch\n        evaluation_strategy=\"steps\",  # Evaluate after every epoch\n        eval_steps=steps_per_epoch,  # Evaluate every epoch\n        do_eval=True,\n        report_to=\"wandb\",  # Logs to WandB for monitoring\n        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    callbacks=[PrintLossCallback]  # Add the custom callback to print loss\n)\n\n# Disable cache for training\nmodel.config.use_cache = False\n\n# Start training\ntrainer.train()\n\n# Print out the model and tokenizer saving path\nprint(f\"Model and tokenizer saved to {output_dir}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T08:34:54.061304Z","iopub.execute_input":"2025-01-01T08:34:54.061544Z","execution_failed":"2025-01-01T15:16:25.233Z"}},"outputs":[{"name":"stdout","text":"steps per epoch: 164\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='907' max='984' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [907/984 4:47:00 < 24:25, 0.05 it/s, Epoch 5.52/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>164</td>\n      <td>0.933600</td>\n      <td>0.690677</td>\n    </tr>\n    <tr>\n      <td>328</td>\n      <td>0.592800</td>\n      <td>0.661081</td>\n    </tr>\n    <tr>\n      <td>492</td>\n      <td>0.428300</td>\n      <td>0.694747</td>\n    </tr>\n    <tr>\n      <td>656</td>\n      <td>0.283200</td>\n      <td>0.776974</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.178900</td>\n      <td>0.853082</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.9336\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5928\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4283\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2832\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1789\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}